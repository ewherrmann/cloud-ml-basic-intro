{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Launch Jupyter\n",
    "\n",
    "You made it here, so great job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Getting Familiar with Jupyter Notebook\n",
    "Jupyter Notebook is one of the most popular development environments for data scientists. As the name implies, it's meant to be a notebook of sorts where you can quickly jot down ideas, experiment, and see results. It's not typically used for production; although, there are projects available that aim to make notebooks accessible in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Markdown Cell\n",
    "\n",
    "1. Double-click inside this cell to toggle to edit mode\n",
    "0. Run cell to toggle back to markdown preview mode\n",
    "\n",
    "A notebook contains individual cells which can be used either for an executable block of code or markdown enabled text for notes. Each cell can be toggled between Code and Markdown from the menu. \n",
    "\n",
    "This cell is set to Markdown and can be formatted with **bold**, *italic*, <font color=\"blue\">colors</font>, etc. This allows for creation of beautifully documented live experimentation and analysis. To view in Markdown mode, highlight the cell, and click \"Run\" button on the top menu or by using `SHIFT`+`RETURN` on your keyboard. Double click anywhere in the cell to toggle back to edit mode.\n",
    "\n",
    "For more information on Markdown https://www.markdownguide.org/cheat-sheet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT AND RUN THIS CODE CELL\n",
    "##This is a code cell used to create and execute blocks of python code. To execute this code and view output, select and run this cell.\n",
    "\n",
    "# Use the print statement to show output\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can change the code and rerun it to see any changed output.<br>\n",
    "- You can also clear output from menu by highlighting cell and then from menu using<br>`Cell->Current Outputs->Clear`\n",
    "\n",
    "Go ahead and play around with the next few code cells to familiarize yourself with Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print can also be used with functions\n",
    "foo = \"hello\"\n",
    "print(\" \".join([foo, \"world\"]))\n",
    "\n",
    "# Not just for strings\n",
    "print(1+3)\n",
    "\n",
    "# The last line to be evaluated will also be shown in the output\n",
    "f'The length of foo is {len(foo)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables set in a cell are available throughout the notebook, not just the cell in which they are created. However, individual cells do not have to be run from top to bottom or in any certain order, so the variable will only be available if the cell has been ran/executed in the current session.\n",
    "\n",
    "Run the cell below and you will see that `foo` can be used, but trying to access `bar` will throw an error since it has yet to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(foo)\n",
    "print(bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below and then rerun the one above and see that `bar` is now available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = \"world\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a quick and relatively shallow overview of Jupyter Notebooks, but it should be enough to help you start navigating and getting hands-on with the fun stuff. To find out more, visit https://jupyter.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Install NumPy\n",
    "\n",
    "If you are running this tutorial locally, you can use `pip install` from your command line. Installing packages from within a notebook directly is not generally a great idea, but we will do it for purposes of this tutorial especially since we are using mybinder.org which is a temporary notebook server generated from a notebook on a github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL TO INSTALL NUMPY\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is heavily used in python when dealing with data science and machine learning projects. It's a super efficient tool for creating and processing multi dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMPLE ARRAY CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "simple_array = np.array([5, 10])\n",
    "print(f'simple array contents: {simple_array}')\n",
    "print(f'simple array shape: {simple_array.shape}')\n",
    "print(f'simple array cell content by index: {simple_array[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy's strength lies within it's ability to process large, multi-dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTI-DIMENSIONAL ARRAY CODE HERE\n",
    "import string\n",
    "multi_array = np.array([list(string.ascii_lowercase), list(range(26))], dtype=object)\n",
    "print(f'multi array contents: {multi_array}')\n",
    "print(f'multi array shape: {multi_array.shape}')\n",
    "print(f'multi array cell content by index: {multi_array[0][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpy arrays are optimized for memory usage and size is defined up at declaration. If you want to append an unknown amount of items dynamically, a native Python structure may be a better fit\n",
    "- Numpy has many powerful mathematical functions that can be performed on arrays\n",
    "- Find more info https://numpy.org/doc/stable/user/quickstart.html#the-basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is like a small, in-memory database that allows for processing data. Internally, it utilizes numpy, but adds some convenient query and data access. One very common usage is to import and export CSV files to and from a Pandas DataFrame. Pandas has a domain specific language to itself, so you should keep the documentation handy! https://pandas.pydata.org/docs/reference/\n",
    "\n",
    "The two main Pandas data structures are Series and DataFrame. A Series is a labeled one-dimensional array and a DataFrame is a two dimensional array with rows, columns, and labels. All individual rows and columns of a DataFrame are made up of individual Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALL AND IMPORT PANDAS\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE SERIES FROM LIST CODE HERE\n",
    "egeeks = ['Thomas','Rich','Craig','Dan','Ed']\n",
    "ring_srs = pd.Series([12,8,10,8,13],index=egeeks, name='Ring Size')\n",
    "print(\"Series type:\", type(ring_srs))\n",
    "print(\"Series name:\", ring_srs.name)\n",
    "print(f\"Rich's ring size is {ring_srs['Rich']}\")\n",
    "ring_srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE SERIES FROM DICTIONARY CODE HERE\n",
    "state_srs = pd.Series({\"Thomas\": \"Indiana\", \"Rich\": \"Pennsylvania\", \"Craig\": \"North Rhine-Westphalia\", \"Dan\": \"New York\", \"Ed\":\"Indiana\"}, name=\"State\")\n",
    "print(\"Series name:\", state_srs.name)\n",
    "print(\"Series type:\", type(state_srs))\n",
    "print(f\"Thomas lives in {state_srs['Thomas']}\")\n",
    "state_srs.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME FROM SERIES CODE HERE\n",
    "egeeks_df = pd.concat([ring_srs, state_srs], axis=1) \n",
    "print(type(egeeks_df))\n",
    "print(f\"Rich lives in {egeeks_df.loc['Rich']['State']}\")\n",
    "egeeks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these cells to see more about the characteristics and capabilities of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#individual row of a DataFrame is a Series\n",
    "print(egeeks_df.loc['Thomas'])\n",
    "print(type(egeeks_df.loc['Thomas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#individual column of a DataFrame is a Series\n",
    "print(egeeks_df['Ring Size'])\n",
    "print(type(egeeks_df['Ring Size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add another column directly from list\n",
    "egeeks_df['City'] = ['Jasper', 'York', 'Lippstadt', 'New York', 'New Albany']\n",
    "print(egeeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column with single value\n",
    "egeeks_df['Favorite Language'] = 'ABAP'\n",
    "print(egeeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our egeeks data to csv\n",
    "egeeks_df.to_csv('egeeks_data.csv', index_label='Name') #give index a label for csv column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our egeeks data from csv\n",
    "egeeks_df = None\n",
    "egeeks_df = pd.read_csv('egeeks_data.csv', index_col='Name') #specify which column to use as index\n",
    "print(egeeks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframes are queryable!\n",
    "print(\"egeeks living in Indiana:\")\n",
    "print(egeeks_df.loc[egeeks_df['State'] == 'Indiana'])\n",
    "print(\"\\negeeks with small fingers:\")\n",
    "print(egeeks_df.loc[egeeks_df['Ring Size'] <= 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with some of the basic tools, it's time to dive into machine learning. In the following steps, you will build a machine learning model that will predict if a line of text is more likely a quote from a `Star Wars` character or `Elon Musk`. May the force be with you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Install Scikit-Learn\n",
    "For this tutorial, you will be using scikit-learn, a popular machine learning library for python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALL SCIKIT-LEARN\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Prepare Elon Musk Data\n",
    "\n",
    "The data files that will be used, are already available in this notebook server. To view all the available files, including the `.ipynb` notebook file itself, use menu option `File->Open`.\n",
    "\n",
    "The Elon Musk dataset contains tweets and retweets made by `@elonmusk` between November 2012 and September 2017. Source: https://www.kaggle.com/kulgen/elon-musks-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MUSK DATA\n",
    "elon_df = pd.read_csv('data_elonmusk.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREVIEW MUSK DATA\n",
    "print('Elon Tweets and Retweets:', len(elon_df))\n",
    "elon_df.head(20) #show first 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE NEW DATAFRAME WITH NO RETWEETS\n",
    "elon_clean_df = pd.DataFrame(elon_df.loc[elon_df['Retweet from'].isnull()]['Tweet'].tolist(), columns=['text',]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN UP MUSK DATA FOR PROCESSING\n",
    "elon_clean_df['dialogue'] = elon_clean_df['text']\n",
    "#remove URLs using regex\n",
    "elon_clean_df['dialogue'] = elon_clean_df['dialogue'].str.replace(r\"http\\S+\", \"\")\n",
    "elon_clean_df['dialogue'] = elon_clean_df['dialogue'].str.replace(r\"http\", \"\")\n",
    "#remove @ mentions using regex\n",
    "elon_clean_df['dialogue'] = elon_clean_df['dialogue'].str.replace(r\"@\\S+\", \"\")\n",
    "#replace any line breaks with spaces \n",
    "elon_clean_df['dialogue'] = elon_clean_df['dialogue'].str.replace(r\"\\n\", \" \")\n",
    "elon_clean_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREVIEW PREPROCESSED MUSK DATA\n",
    "print('Elon Tweets', len(elon_clean_df))\n",
    "elon_clean_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Prepare Star Wars Data\n",
    "The data files are movie lines from the original trilogy. Source: https://www.kaggle.com/xvivancos/star-wars-movie-scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD MOVIE DATA\n",
    "hope_df = pd.read_csv('SW_EpisodeIV.txt', delim_whitespace=True, names=[\"id\",\"character\",\"dialogue\"], index_col=\"id\", skiprows=[0,])\n",
    "emp_df = pd.read_csv('SW_EpisodeV.txt', delim_whitespace=True, names=[\"id\",\"character\",\"dialogue\"], index_col=\"id\", skiprows=[0,])\n",
    "jedi_df = pd.read_csv('SW_EpisodeVI.txt', delim_whitespace=True, names=[\"id\",\"character\",\"dialogue\"], index_col=\"id\", skiprows=[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE NEW DATAFRAME WITH ALL MOVIE LINES\n",
    "sw_df = pd.concat([hope_df, emp_df, jedi_df]) \n",
    "print('Total Star Wars movie lines: ', len(sw_df))\n",
    "sw_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD COLUMN TO BOTH DATAFRAMES TO INDICATE SOURCE\n",
    "sw_df['who said it'] = 'Star Wars'\n",
    "elon_clean_df['who said it'] = 'Elon Musk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMBINE INTO SINGLE DATAFRAME\n",
    "all_df = pd.concat([elon_clean_df[['dialogue', 'who said it']], sw_df[['dialogue', 'who said it']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREVIEW PREPROCESSED COMBINED FULL DATASET\n",
    "print(\"Total lines of dialogue:\", len(all_df))\n",
    "all_df.head(10).append(all_df.tail(10)) #show a preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Vectorization - Fit\n",
    "Machines like numbers, not text. In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors. There are many different methods and theories for vectorization and some options are better depending on the use case. For this tutorial, we will use TF-IDF (Term Frequency times Inverse Document Frequency).\n",
    "\n",
    "TF-IDF is a way of adjusting the importance or weight of individual terms in relation to all of the terms and frequencies of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VECTORIZATION FIT CODE HERE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_vectorizer.fit(all_df['dialogue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer is fit with the whole dataset, which now allows for new or existing lines of text to be mapped into vectors for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show vector representation of text line\n",
    "tfidf_vectorizer.transform([\"may the force be with you\",])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Shuffle and Split Data\n",
    "It is important to hold back some data to test the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHUFFLE AND SPLIT CODE HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_df_train, all_df_test = train_test_split(all_df, test_size=0.20) #split 20% for testing\n",
    "print(\"All data:\", len(all_df))\n",
    "print(\"Train data:\", len(all_df_train))\n",
    "print(\"Test data:\", len(all_df_test))\n",
    "all_df_test.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice...data is split and shuffled with 80% for training and 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Vectorization - Transform the Training Data\n",
    "In the previous vectorization step, the vectorizer object was fit with the entire dataset to create a map of terms. Now you will transform each individual line of text from the training data into a vector for fitting the text classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORM TRAINING DATA CODE \n",
    "dialogue_tfidf_train = tfidf_vectorizer.transform(all_df_train['dialogue'])\n",
    "dialogue_tfidf_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the result is a matrix of 224 lines of vectorized data, one for each line in our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Train the Model\n",
    "Time to get into the fun stuff, training a machine learning model! Lets train a simple sklearn Naive Bayes classifier.\n",
    "\n",
    "Now that the training data has been vectorized, it's time to train/fit the model. The input for the following `fit` method is 2 iterable arrays, one for the vectorized representation of the text and the other for the target value of `Star Wars` or `Elon Musk`. It is expected that the arrays are the same size and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN CLASSIFIER MODEL\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sw_elon_clf = MultinomialNB().fit(dialogue_tfidf_train, all_df_train['who said it'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really is that it? Yep, your machine learning model has been trained...amazing, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Predict the Test Data\n",
    "After the model has been trained, it can be used to predict the target values of other data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS JUST TO TEMPORARILY IGNORE WARNINGS WHILE WE PREVIEW DATAFRAME SLICES\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PREDICT TEST DATA CODE HERE \n",
    "#vectorize test data\n",
    "dialogue_tfidf_test = tfidf_vectorizer.transform(all_df_test['dialogue'])\n",
    "#run predictions for our test data\n",
    "all_df_test['who said it prediction'] = sw_elon_clf.predict(dialogue_tfidf_test)\n",
    "#preview predictions\n",
    "all_df_test.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Metrics\n",
    "To measure accuracy, use the test predictions and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY\n",
    "from sklearn import metrics\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(all_df_test['who said it'], all_df_test['who said it prediction']) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX\n",
    "cm = metrics.confusion_matrix(all_df_test['who said it'], all_df_test['who said it prediction'], labels=['Elon Musk', 'Star Wars'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Chart Confusion Matrix\n",
    "The confusion matrix shows our agreements and disagreements for both target values. To make it more clear, we can use matplotlib to generate a more user friendly version of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALL MATPLOTLIB\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD CHART CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=\"RdYlGn\")\n",
    "plt.tight_layout()\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title('Who Said It Confusion Matrix', fontsize=20)\n",
    "plt.xlabel('Predicted', fontsize=20)\n",
    "plt.ylabel('Actual', fontsize=20)\n",
    "\n",
    "classes = ['Elon Musk', 'Star Wars']\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, fontsize=20, rotation=45)\n",
    "plt.yticks(tick_marks, classes, fontsize=20)\n",
    "\n",
    "#iterate through matrix and plot values\n",
    "for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\", fontsize=40)\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows:\n",
    "* Top left: # of predictions where the actual and predicted value was Elon Musk\n",
    "* Top right: # of predictions where the actual value was Elon Musk but the predicted value was Star Wars\n",
    "* Bottom left: # of predictions where the actual value was Star Wars but the predicted value was Elon Musk\n",
    "* Bottom right: # of predictions where both the actual and predicted value was Star Wars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Build Pipeline\n",
    "Once the model is built, you will most likely want to use it to predict and classify future lines of dialogue. It's important that you preprocess and vectorize any future data the same way in which it was built before feeding it in for classification. Scikit-Learn provides the concept of pipelines that are objects that can store all the necessary steps to process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can see here that you would have to run each line of text through the vectorization process before a prediction\n",
    "test_line = \"may the force be with you\"\n",
    "test_line_tfidf = tfidf_vectorizer.transform([test_line,])\n",
    "sw_elon_clf.predict(test_line_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD PIPELINE CODE HERE\n",
    "from sklearn.pipeline import Pipeline\n",
    "sw_elon_clf_pipe = Pipeline([\n",
    "    ('vect', tfidf_vectorizer),\n",
    "    ('clf', sw_elon_clf),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST A PREDICTION\n",
    "test_line = \"may the force be with you\"\n",
    "sw_elon_clf_pipe.predict([test_line,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE PIPELINE OBJECT\n",
    "import joblib\n",
    "joblib.dump(sw_elon_clf_pipe, 'sw_elon_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 18: Ad Hoc Predictions\n",
    "Now that the pipeline has been built, use it to run some ad hoc predictions. Have fun with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD PIPELINE AND RUN AD HOC PREDICTIONS\n",
    "sw_elon_clf_pipe = joblib.load('sw_elon_model.pkl')\n",
    "test_lines = [\n",
    "    'may the force be with you', \n",
    "    'i love tesla', \n",
    "    'i am your father', \n",
    "    'seagulls stop it now', \n",
    "    'the next SAP teched should be on Mars',\n",
    "    'come to the dark side, we have cookies',\n",
    "]\n",
    "\n",
    "test_predictions = sw_elon_clf_pipe.predict(test_lines)\n",
    "\n",
    "test_results_df = pd.DataFrame({\"Text\": test_lines, \"Prediction\": test_predictions})\n",
    "test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, Elon Musk loves SAP TechEd and Star Wars loves cookies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 19: BONUS: UI for Fun\n",
    "As a fun bonus, create a quick UI in Jupyter notebook that will allow you to input some text, hit enter, and show the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BONUS FUN UI CODE HERE\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "text_box = widgets.Text()\n",
    "\n",
    "print(\"Who is most likely to say:\")\n",
    "display(text_box)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(f\"'{text_box.value}':\", sw_elon_clf_pipe.predict([text_box.value,])[0])\n",
    "    \n",
    "text_box.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type in some text and hit enter to see who was more likely to say it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 20: The End - Congrats!\n",
    "Wow, a fully functional, production like machine learning toy app! Hopefully you are now equipped with the necessary knowledge to begin your own adventure in Data Science and Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
